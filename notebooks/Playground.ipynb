{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN\n",
    "This code receives multiple timeseries, transforms them into graphs, and then applies a GNN to them.\n",
    "The graph embeddings are then used for downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 3\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from src.b2bnet import B2BNetModel, RandomDataModule, OtkaDataModule\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/b2bnet/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder     | RNN              | 1.3 K \n",
      "1 | fc          | Linear           | 6     \n",
      "2 | decoder     | RNN              | 704 K \n",
      "3 | b2b         | RNN              | 704 K \n",
      "4 | fc_text     | Linear           | 3     \n",
      "5 | loss_cls    | CrossEntropyLoss | 0     \n",
      "6 | loss_reconn | MSELoss          | 0     \n",
      "7 | loss_b2b    | MSELoss          | 0     \n",
      "8 | loss_text   | MSELoss          | 0     \n",
      "-------------------------------------------------\n",
      "1.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 M     Total params\n",
      "5.642     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85eca54a342a40daab515083d7e61315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/b2bnet/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/b2bnet/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/b2bnet/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66da50badee545288a3916c5d3b8202b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/b2bnet/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/b2bnet/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7ba980e96c46c9a34c466adb10b2df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f7aa2b1f43a4525b370f1bd6bbbda79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    }
   ],
   "source": [
    "# VISUALIZE model's graph\n",
    "datamodule = OtkaDataModule()\n",
    "model = B2BNetModel(input_size=59, n_timesteps=100, n_cls_labels=2, text_dim=1)\n",
    "\n",
    "# TRAINING\n",
    "trainer = pl.Trainer(max_epochs=2, accelerator='cpu', log_every_n_steps=1)\n",
    "\n",
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 3\n",
    "\n",
    "import torch\n",
    "from src.b2bnet import TCN\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 59, 40000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "n_timesteps = 40000\n",
    "n_features = 59\n",
    "X = torch.randn(batch_size, n_timesteps, n_features).permute(0, 2, 1)\n",
    "model = TCN(n_timesteps, output_length=128, n_features=n_features, kernel_size=2, dilation_base=2)\n",
    "model(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/morteza/micromamba/envs/b2bnet/lib/python3.10/site-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
      "/Users/morteza/micromamba/envs/b2bnet/lib/python3.10/site-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return super().__sizeof__() + self.nbytes()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TCN                                      [32, 59, 40000]           --\n",
       "├─Sequential: 1-1                        [32, 59, 40000]           --\n",
       "│    └─TCNBlock: 2-1                     [32, 59, 40000]           --\n",
       "│    │    └─Sequential: 3-1              [32, 59, 40000]           7,021\n",
       "│    └─TCNBlock: 2-2                     [32, 59, 40000]           --\n",
       "│    │    └─Sequential: 3-2              [32, 59, 40000]           7,021\n",
       "│    └─TCNBlock: 2-3                     [32, 59, 40000]           --\n",
       "│    │    └─Sequential: 3-3              [32, 59, 40000]           7,021\n",
       "│    └─TCNBlock: 2-4                     [32, 59, 40000]           --\n",
       "│    │    └─Sequential: 3-4              [32, 59, 40000]           7,021\n",
       "│    └─TCNBlock: 2-5                     [32, 59, 40000]           --\n",
       "│    │    └─Sequential: 3-5              [32, 59, 40000]           7,021\n",
       "│    └─TCNBlock: 2-6                     [32, 59, 40000]           --\n",
       "│    │    └─Sequential: 3-6              [32, 59, 40000]           7,021\n",
       "│    └─TCNBlock: 2-7                     [32, 59, 40000]           --\n",
       "│    │    └─Sequential: 3-7              [32, 59, 40000]           7,021\n",
       "│    └─TCNBlock: 2-8                     [32, 59, 40000]           --\n",
       "│    │    └─Sequential: 3-8              [32, 59, 40000]           7,021\n",
       "│    └─TCNBlock: 2-9                     [32, 59, 40000]           --\n",
       "│    │    └─Sequential: 3-9              [32, 59, 40000]           7,021\n",
       "│    └─TCNBlock: 2-10                    [32, 59, 40000]           --\n",
       "│    │    └─Sequential: 3-10             [32, 59, 40000]           7,021\n",
       "│    └─TCNBlock: 2-11                    [32, 59, 40000]           --\n",
       "│    │    └─Sequential: 3-11             [32, 59, 40000]           7,021\n",
       "│    └─TCNBlock: 2-12                    [32, 59, 40000]           --\n",
       "│    │    └─Sequential: 3-12             [32, 59, 40000]           7,021\n",
       "│    └─TCNBlock: 2-13                    [32, 59, 40000]           --\n",
       "│    │    └─Sequential: 3-13             [32, 59, 40000]           7,021\n",
       "│    └─TCNBlock: 2-14                    [32, 59, 40000]           --\n",
       "│    │    └─Sequential: 3-14             [32, 59, 40000]           7,021\n",
       "│    └─TCNBlock: 2-15                    [32, 59, 40000]           --\n",
       "│    │    └─Sequential: 3-15             [32, 59, 40000]           7,021\n",
       "==========================================================================================\n",
       "Total params: 105,315\n",
       "Trainable params: 105,315\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 134.80\n",
       "==========================================================================================\n",
       "Input size (MB): 302.08\n",
       "Forward/backward pass size (MB): 9062.40\n",
       "Params size (MB): 0.42\n",
       "Estimated Total Size (MB): 9364.90\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(batch_size, n_features, n_timesteps))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "b2bnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
